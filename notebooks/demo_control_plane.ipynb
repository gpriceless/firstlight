{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FirstLight LLM Control Plane -- MAIA Analytics Demo\n",
    "\n",
    "**Geospatial Event Intelligence, steered by AI.**\n",
    "\n",
    "---\n",
    "\n",
    "FirstLight converts **(area, time window, event type)** into actionable decision products\n",
    "-- flood extent maps, damage assessments, infrastructure impact reports -- using satellite imagery\n",
    "and contextual data.\n",
    "\n",
    "What makes FirstLight different from a traditional geospatial pipeline is its **LLM Control Plane**:\n",
    "an API surface purpose-built for AI agents to **discover**, **submit**, **steer**, and **audit**\n",
    "analysis jobs without human intervention.\n",
    "\n",
    "This notebook walks through the full platform capability using a **Houston, TX flood detection**\n",
    "scenario.\n",
    "\n",
    "### What We Will Cover\n",
    "\n",
    "| Section | Capability | Why It Matters |\n",
    "|---------|-----------|----------------|\n",
    "| 1 | Setup & Health Check | Verify the deployment is live |\n",
    "| 2 | LLM Router (Tool Discovery) | AI agents discover algorithms at runtime -- no hardcoded knowledge |\n",
    "| 3 | Job Creation | Submit analysis with AOI geometry + LLM reasoning |\n",
    "| 4 | Pipeline Phase Transitions | Atomic state machine with TOCTOU concurrency guards |\n",
    "| 5 | LLM Reasoning Injection | AI records its chain of thought with confidence scores |\n",
    "| 6 | Parameter Tuning | Adjust algorithms mid-flight via JSON merge-patch |\n",
    "| 7 | Escalation Workflow | Human-in-the-loop when AI confidence is low |\n",
    "| 8 | Context Lakehouse | Spatial queries across accumulated satellite data, buildings, infrastructure, weather |\n",
    "| 9 | Partner Integration (Metrics & Queue) | Pipeline health dashboard data |\n",
    "| 10 | SSE Event Stream | Real-time event delivery with CloudEvents v1.0 |\n",
    "| 11 | Standards: OGC & STAC | Interoperability with GIS tools and open geospatial standards |\n",
    "| 12 | Summary | Architecture recap and endpoint map |\n",
    "\n",
    "### API Surfaces\n",
    "\n",
    "| Prefix | Audience | Purpose |\n",
    "|--------|----------|--------|\n",
    "| `/control/v1/*` | LLM Agents (MAIA) | Job lifecycle, reasoning, escalations, tool discovery |\n",
    "| `/internal/v1/*` | Partner Backend | SSE events, webhooks, metrics, queue status |\n",
    "| `/oapi/*` | GIS Tools | OGC API Processes -- standards-compliant interface |\n",
    "| `/stac/*` | Any Client | STAC Catalog -- discover published analysis results |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 1. Setup & Configuration\n\nConfigure the API base URL, authentication, and helper functions.\n\nAuthentication uses the **`X-API-Key`** header -- a simple, stateless key-based auth\nthat maps to a tenant context on the server side. Each API key is scoped to a single\ncustomer, ensuring multi-tenant isolation at the middleware layer."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport requests\nfrom datetime import datetime, timezone\nfrom IPython.display import display, Markdown, HTML\n\n# ---------------------------------------------------------------------------\n# Configuration\n# ---------------------------------------------------------------------------\nBASE_URL = \"http://localhost:8000\"\nAPI_KEY = \"demo-18254ee7d18f5926\"\n\nHEADERS = {\n    \"X-API-Key\": API_KEY,\n    \"Content-Type\": \"application/json\",\n}\n\n# ---------------------------------------------------------------------------\n# Helper functions\n# ---------------------------------------------------------------------------\n\ndef api_get(path, params=None):\n    \"\"\"Authenticated GET request. Returns (status_code, parsed_json_or_text).\"\"\"\n    url = f\"{BASE_URL}{path}\"\n    try:\n        resp = requests.get(url, headers=HEADERS, params=params, timeout=30)\n        try:\n            return resp.status_code, resp.json()\n        except ValueError:\n            return resp.status_code, resp.text\n    except requests.exceptions.ConnectionError as e:\n        print(f\"ERROR: Cannot connect to {url}\")\n        print(f\"Is the FirstLight API running? Start with: uvicorn api.main:app\")\n        return 0, None\n\n\ndef api_post(path, body=None):\n    \"\"\"Authenticated POST request.\"\"\"\n    url = f\"{BASE_URL}{path}\"\n    try:\n        resp = requests.post(url, headers=HEADERS, json=body, timeout=30)\n        try:\n            return resp.status_code, resp.json()\n        except ValueError:\n            return resp.status_code, resp.text\n    except requests.exceptions.ConnectionError as e:\n        print(f\"ERROR: Cannot connect to {url}\")\n        return 0, None\n\n\ndef api_patch(path, body=None):\n    \"\"\"Authenticated PATCH request.\"\"\"\n    url = f\"{BASE_URL}{path}\"\n    try:\n        resp = requests.patch(url, headers=HEADERS, json=body, timeout=30)\n        try:\n            return resp.status_code, resp.json()\n        except ValueError:\n            return resp.status_code, resp.text\n    except requests.exceptions.ConnectionError as e:\n        print(f\"ERROR: Cannot connect to {url}\")\n        return 0, None\n\n\ndef api_delete(path):\n    \"\"\"Authenticated DELETE request.\"\"\"\n    url = f\"{BASE_URL}{path}\"\n    try:\n        resp = requests.delete(url, headers=HEADERS, timeout=30)\n        if resp.status_code == 204:\n            return 204, None\n        try:\n            return resp.status_code, resp.json()\n        except ValueError:\n            return resp.status_code, resp.text\n    except requests.exceptions.ConnectionError as e:\n        print(f\"ERROR: Cannot connect to {url}\")\n        return 0, None\n\n\ndef pp(data):\n    \"\"\"Pretty-print JSON data.\"\"\"\n    if data is not None:\n        print(json.dumps(data, indent=2, default=str))\n    else:\n        print(\"(no data)\")\n\n\nprint(f\"Target API:  {BASE_URL}\")\nprint(f\"API Key:     {API_KEY[:12]}...\")\nprint(f\"Timestamp:   {datetime.now(timezone.utc).isoformat()}\")\nprint(\"\\nReady.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Health Check\n",
    "\n",
    "Verify the API is reachable and responsive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_code, health = api_get(\"/api/v1/health\")\n",
    "print(f\"HTTP {status_code}\")\n",
    "pp(health)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. LLM Router: Tool Discovery\n",
    "\n",
    "**`GET /control/v1/tools`**\n",
    "\n",
    "This is where the LLM Control Plane starts. Before an AI agent can submit a job, it needs to\n",
    "know **what analysis algorithms are available** and **what parameters they accept**.\n",
    "\n",
    "FirstLight returns tool schemas in **OpenAI function-calling format** -- the same JSON schema\n",
    "structure used by GPT-4, Claude, Gemini, and other LLMs for native tool use. This means MAIA\n",
    "can discover FirstLight's capabilities at runtime without any hardcoded knowledge of the platform.\n",
    "\n",
    "> **Why This Matters:** Traditional geospatial platforms require custom integrations per algorithm.\n",
    "> With the LLM Router, adding a new algorithm to FirstLight automatically makes it available\n",
    "> to every connected AI agent -- zero integration work on the partner side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_code, tools_response = api_get(\"/control/v1/tools\")\n",
    "print(f\"HTTP {status_code}\\n\")\n",
    "\n",
    "if status_code == 200 and tools_response:\n",
    "    tools = tools_response.get(\"tools\", [])\n",
    "    print(f\"Discovered {len(tools)} tool(s):\\n\")\n",
    "    \n",
    "    for tool in tools:\n",
    "        print(f\"  Name:        {tool.get('name')}\")\n",
    "        print(f\"  Description: {tool.get('description', '')[:120]}\")\n",
    "        params = tool.get('parameters', {})\n",
    "        prop_names = list(params.get('properties', {}).keys())\n",
    "        if prop_names:\n",
    "            print(f\"  Parameters:  {', '.join(prop_names)}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"Could not retrieve tools.\")\n",
    "    pp(tools_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How an LLM Uses These Schemas\n",
    "\n",
    "Each tool schema follows the OpenAI function-calling convention. Here is how it plugs into\n",
    "an LLM's tool-use system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the full schema of the first tool as an example\n",
    "if tools_response and tools_response.get(\"tools\"):\n",
    "    example_tool = tools_response[\"tools\"][0]\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXAMPLE: Full tool schema (OpenAI function-calling format)\")\n",
    "    print(\"=\" * 70)\n",
    "    pp(example_tool)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"How this would appear in an LLM API call:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Show how this maps to the OpenAI tools parameter\n",
    "    openai_format = {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": example_tool\n",
    "    }\n",
    "    print(\"\\n# In an OpenAI/Claude API call, this tool schema becomes:\")\n",
    "    pp(openai_format)\n",
    "    \n",
    "    print(\"\\n# The LLM would call it like:\")\n",
    "    print(f'# tool_call(name=\"{example_tool[\"name\"]}\", arguments={{...}})')\n",
    "else:\n",
    "    print(\"No tools available to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Create an Analysis Job\n",
    "\n",
    "**`POST /control/v1/jobs`**\n",
    "\n",
    "### Scenario: Houston Flood Detection\n",
    "\n",
    "MAIA's NLP pipeline has detected flood-related NOAA weather alerts in the Houston, TX area.\n",
    "Multiple river gauges along Buffalo Bayou are reporting above-flood-stage water levels.\n",
    "\n",
    "The AI agent creates a flood detection job covering the **Houston Ship Channel / Buffalo Bayou**\n",
    "area -- approximately 25 km2 of historically flood-prone terrain.\n",
    "\n",
    "The AOI (Area of Interest) is a GeoJSON Polygon in **WGS84 (EPSG:4326)**.\n",
    "\n",
    "> **Why This Matters:** The job creation request carries not just the technical parameters,\n",
    "> but also the AI's **reasoning** for initiating the analysis. This creates an audit trail\n",
    "> from the very first decision -- why did the AI think a flood analysis was needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Houston Ship Channel / Buffalo Bayou AOI\n",
    "# WGS84 (EPSG:4326), ~25 km2 of historically flood-prone area\n",
    "HOUSTON_AOI = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [[\n",
    "        [-95.3698, 29.7604],   # NW corner -- near downtown Houston\n",
    "        [-95.2900, 29.7604],   # NE corner -- east of Ship Channel\n",
    "        [-95.2900, 29.7100],   # SE corner -- south of Turning Basin\n",
    "        [-95.3698, 29.7100],   # SW corner\n",
    "        [-95.3698, 29.7604],   # close the ring\n",
    "    ]],\n",
    "}\n",
    "\n",
    "create_body = {\n",
    "    \"event_type\": \"flood\",\n",
    "    \"aoi\": HOUSTON_AOI,\n",
    "    \"parameters\": {\n",
    "        \"sensitivity\": \"medium\",\n",
    "        \"min_area_km2\": 0.1,\n",
    "        \"include_sar\": True,\n",
    "    },\n",
    "    \"reasoning\": (\n",
    "        \"Detected potential flood event in Houston area based on \"\n",
    "        \"NOAA Weather Alert WEA-2026-0451. Multiple river gauges \"\n",
    "        \"reporting above-flood-stage levels along Buffalo Bayou. \"\n",
    "        \"Initiating SAR and optical flood detection analysis.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(\"Request body:\")\n",
    "pp(create_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_code, job_response = api_post(\"/control/v1/jobs\", create_body)\n",
    "print(f\"HTTP {status_code}\\n\")\n",
    "pp(job_response)\n",
    "\n",
    "# Extract the job_id for use in subsequent calls\n",
    "JOB_ID = None\n",
    "if status_code == 201 and job_response:\n",
    "    JOB_ID = job_response.get(\"job_id\")\n",
    "    print(f\"\\nJob ID: {JOB_ID}\")\n",
    "    print(f\"Phase:  {job_response.get('phase')}\")\n",
    "    print(f\"Status: {job_response.get('status')}\")\n",
    "else:\n",
    "    print(\"\\nJob creation failed. Check API connectivity and authentication.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Full Job Detail\n",
    "\n",
    "**`GET /control/v1/jobs/{job_id}`**\n",
    "\n",
    "The detail view includes the stored AOI geometry, computed area in km2, current parameters,\n",
    "and hypermedia links to related resources (events, checkpoints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if JOB_ID:\n",
    "    status_code, detail = api_get(f\"/control/v1/jobs/{JOB_ID}\")\n",
    "    print(f\"HTTP {status_code}\\n\")\n",
    "    pp(detail)\n",
    "else:\n",
    "    print(\"No job ID available -- skipping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Pipeline Phase Transitions\n",
    "\n",
    "**`POST /control/v1/jobs/{job_id}/transition`**\n",
    "\n",
    "A job moves through seven phases:\n",
    "\n",
    "```\n",
    "QUEUED --> DISCOVERING --> INGESTING --> NORMALIZING --> ANALYZING --> REPORTING --> COMPLETE\n",
    "```\n",
    "\n",
    "Each transition is **atomic** and protected by a **TOCTOU (Time-of-Check-Time-of-Use) guard**.\n",
    "The caller must specify the **expected** current state and the **target** state. If another actor\n",
    "(another AI agent, a human operator, or the pipeline itself) has already moved the job, the\n",
    "transition fails with **HTTP 409 Conflict** instead of silently corrupting state.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"expected_phase\": \"QUEUED\",\n",
    "  \"expected_status\": \"PENDING\",\n",
    "  \"target_phase\": \"QUEUED\",\n",
    "  \"target_status\": \"VALIDATING\",\n",
    "  \"reason\": \"Starting input validation\"\n",
    "}\n",
    "```\n",
    "\n",
    "> **Why This Matters:** In a multi-agent system, multiple AI agents might try to advance the\n",
    "> same job simultaneously. The TOCTOU guard ensures exactly-once state transitions -- the\n",
    "> foundation for reliable distributed pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the full transition path from QUEUED/PENDING through to ANALYZING/ANALYZING\n",
    "# We will complete the pipeline in a later section after reasoning and escalation.\n",
    "\n",
    "TRANSITIONS = [\n",
    "    {\n",
    "        \"expected_phase\": \"QUEUED\", \"expected_status\": \"PENDING\",\n",
    "        \"target_phase\": \"QUEUED\", \"target_status\": \"VALIDATING\",\n",
    "        \"reason\": \"Validating input parameters and AOI geometry\",\n",
    "    },\n",
    "    {\n",
    "        \"expected_phase\": \"QUEUED\", \"expected_status\": \"VALIDATING\",\n",
    "        \"target_phase\": \"QUEUED\", \"target_status\": \"VALIDATED\",\n",
    "        \"reason\": \"AOI is a valid WGS84 polygon with reasonable bounds\",\n",
    "    },\n",
    "    {\n",
    "        \"expected_phase\": \"QUEUED\", \"expected_status\": \"VALIDATED\",\n",
    "        \"target_phase\": \"DISCOVERING\", \"target_status\": \"DISCOVERING\",\n",
    "        \"reason\": \"Searching satellite data catalogs (STAC, Copernicus, USGS)\",\n",
    "    },\n",
    "    {\n",
    "        \"expected_phase\": \"DISCOVERING\", \"expected_status\": \"DISCOVERING\",\n",
    "        \"target_phase\": \"DISCOVERING\", \"target_status\": \"DISCOVERED\",\n",
    "        \"reason\": \"Found 3 Sentinel-1 GRD and 2 Sentinel-2 L2A scenes\",\n",
    "    },\n",
    "    {\n",
    "        \"expected_phase\": \"DISCOVERING\", \"expected_status\": \"DISCOVERED\",\n",
    "        \"target_phase\": \"INGESTING\", \"target_status\": \"INGESTING\",\n",
    "        \"reason\": \"Downloading and staging satellite imagery from AWS S3\",\n",
    "    },\n",
    "    {\n",
    "        \"expected_phase\": \"INGESTING\", \"expected_status\": \"INGESTING\",\n",
    "        \"target_phase\": \"INGESTING\", \"target_status\": \"INGESTED\",\n",
    "        \"reason\": \"5 scenes staged, checksums verified (2.3 GB total)\",\n",
    "    },\n",
    "    {\n",
    "        \"expected_phase\": \"INGESTING\", \"expected_status\": \"INGESTED\",\n",
    "        \"target_phase\": \"NORMALIZING\", \"target_status\": \"NORMALIZING\",\n",
    "        \"reason\": \"Band alignment, CRS reprojection to UTM 15N (EPSG:32615)\",\n",
    "    },\n",
    "    {\n",
    "        \"expected_phase\": \"NORMALIZING\", \"expected_status\": \"NORMALIZING\",\n",
    "        \"target_phase\": \"NORMALIZING\", \"target_status\": \"NORMALIZED\",\n",
    "        \"reason\": \"All scenes co-registered, resampled to 10m, radiometrically calibrated\",\n",
    "    },\n",
    "    {\n",
    "        \"expected_phase\": \"NORMALIZING\", \"expected_status\": \"NORMALIZED\",\n",
    "        \"target_phase\": \"ANALYZING\", \"target_status\": \"ANALYZING\",\n",
    "        \"reason\": \"Running flood detection: SAR coherence + NDWI + ML classifier\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Walking through {len(TRANSITIONS)} transitions...\\n\")\n",
    "\n",
    "if JOB_ID:\n",
    "    for i, t in enumerate(TRANSITIONS, 1):\n",
    "        sc, body = api_post(f\"/control/v1/jobs/{JOB_ID}/transition\", t)\n",
    "        phase = body.get(\"phase\", \"?\") if isinstance(body, dict) else \"?\"\n",
    "        status_val = body.get(\"status\", \"?\") if isinstance(body, dict) else \"?\"\n",
    "        \n",
    "        marker = \"OK\" if 200 <= sc < 300 else f\"ERR {sc}\"\n",
    "        print(f\"  [{marker}] {i:2d}. {t['expected_phase']}/{t['expected_status']} \"\n",
    "              f\"--> {phase}/{status_val}\")\n",
    "        print(f\"       Reason: {t['reason']}\")\n",
    "    \n",
    "    print(f\"\\nJob is now at: {phase}/{status_val}\")\n",
    "else:\n",
    "    print(\"No job ID available -- skipping transitions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. LLM Reasoning Injection\n",
    "\n",
    "**`POST /control/v1/jobs/{job_id}/reasoning`**\n",
    "\n",
    "At any point during the job lifecycle, an AI agent can inject its **analytical reasoning** into\n",
    "the event log. Each reasoning entry includes:\n",
    "\n",
    "- **reasoning** -- Free-text chain of thought (up to 64 KB)\n",
    "- **confidence** -- Numeric score (0.0 to 1.0)\n",
    "- **payload** -- Structured data supporting the reasoning\n",
    "\n",
    "These entries become part of the immutable event log alongside state transitions, creating\n",
    "a complete audit trail of both **algorithmic results** and **AI interpretation**.\n",
    "\n",
    "> **Why This Matters:** When a flood map goes to an emergency manager, they need to know\n",
    "> not just *what* the algorithm found, but *why* the AI decided this was significant.\n",
    "> Reasoning entries close that gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning 1: SAR coherence analysis interpretation\n",
    "\n",
    "reasoning_sar = {\n",
    "    \"reasoning\": (\n",
    "        \"SAR coherence analysis indicates significant ground displacement in grid \"\n",
    "        \"cells 4-7, consistent with surface water accumulation. Coherence drop from \"\n",
    "        \"0.85 to 0.23 in the Buffalo Bayou corridor. Cross-referencing with USGS \"\n",
    "        \"gauge station 08074000 confirms water levels 4.2 ft above flood stage. \"\n",
    "        \"Confidence: HIGH. Recommending focused analysis on eastern quadrant where \"\n",
    "        \"displacement is greatest.\"\n",
    "    ),\n",
    "    \"confidence\": 0.85,\n",
    "    \"payload\": {\n",
    "        \"affected_grid_cells\": [4, 5, 6, 7],\n",
    "        \"coherence_drop\": {\"from\": 0.85, \"to\": 0.23},\n",
    "        \"gauge_station\": \"USGS-08074000\",\n",
    "        \"water_level_above_flood_stage_ft\": 4.2,\n",
    "    },\n",
    "}\n",
    "\n",
    "if JOB_ID:\n",
    "    sc, body = api_post(f\"/control/v1/jobs/{JOB_ID}/reasoning\", reasoning_sar)\n",
    "    print(f\"HTTP {sc}\")\n",
    "    pp(body)\n",
    "else:\n",
    "    print(\"No job ID available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition to QUALITY_CHECK substatus\n",
    "\n",
    "if JOB_ID:\n",
    "    sc, body = api_post(f\"/control/v1/jobs/{JOB_ID}/transition\", {\n",
    "        \"expected_phase\": \"ANALYZING\",\n",
    "        \"expected_status\": \"ANALYZING\",\n",
    "        \"target_phase\": \"ANALYZING\",\n",
    "        \"target_status\": \"QUALITY_CHECK\",\n",
    "        \"reason\": \"Running QA checks on detection output\",\n",
    "    })\n",
    "    print(f\"HTTP {sc} -- Transitioned to ANALYZING/QUALITY_CHECK\")\n",
    "    pp(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Parameter Tuning Mid-Flight\n",
    "\n",
    "**`PATCH /control/v1/jobs/{job_id}/parameters`**\n",
    "\n",
    "Based on intermediate results, the AI agent can **adjust algorithm parameters without restarting\n",
    "the job**. This uses **JSON merge-patch** semantics (RFC 7386):\n",
    "\n",
    "- Send only the keys you want to change\n",
    "- Set a key to `null` to remove it\n",
    "- Unmentioned keys are preserved\n",
    "\n",
    "Here, MAIA increases sensitivity and lowers the change threshold after observing strong\n",
    "SAR coherence signals in the eastern quadrant.\n",
    "\n",
    "> **Why This Matters:** Traditional pipelines require resubmission to change parameters.\n",
    "> Mid-flight tuning lets the AI react to intermediate results in real time -- adjusting\n",
    "> the analysis as evidence accumulates, not after the fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_patch = {\n",
    "    \"sensitivity\": \"high\",\n",
    "    \"min_change_threshold\": 0.15,\n",
    "    \"focus_quadrant\": \"NE\",\n",
    "}\n",
    "\n",
    "print(\"Patch body (only changed keys):\")\n",
    "pp(parameter_patch)\n",
    "\n",
    "if JOB_ID:\n",
    "    sc, body = api_patch(f\"/control/v1/jobs/{JOB_ID}/parameters\", parameter_patch)\n",
    "    print(f\"\\nHTTP {sc}\")\n",
    "    print(\"\\nResulting merged parameters:\")\n",
    "    pp(body)\n",
    "else:\n",
    "    print(\"\\nNo job ID available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Escalation Workflow\n",
    "\n",
    "**`POST /control/v1/jobs/{job_id}/escalations`** -- Create escalation  \n",
    "**`PATCH /control/v1/jobs/{job_id}/escalations/{eid}`** -- Resolve escalation\n",
    "\n",
    "When the AI encounters something **outside its confidence threshold**, it escalates to a\n",
    "human operator. Escalations have:\n",
    "\n",
    "- **severity** -- `LOW`, `MEDIUM`, `HIGH`, `CRITICAL`\n",
    "- **reason** -- Why the AI is uncertain\n",
    "- **context** -- Structured data to help the human reviewer\n",
    "\n",
    "The job **continues processing** while awaiting human input -- escalation is not a stop signal,\n",
    "it is a parallel review request.\n",
    "\n",
    "### Scenario\n",
    "\n",
    "MAIA detects an anomalous SAR reflectance pattern in grid cell 6 that could indicate either:\n",
    "- **(a)** Genuine flood extent beyond historical norms, or\n",
    "- **(b)** A SAR sensor artifact from wind-roughened water surface\n",
    "\n",
    "Confidence is 0.62 -- below the 0.75 autonomous reporting threshold.\n",
    "\n",
    "> **Why This Matters:** This is the **human-in-the-loop** pattern done right. The AI does not\n",
    "> stop and wait -- it flags the issue, provides structured context, and lets the human resolve\n",
    "> it when ready. The full escalation lifecycle is recorded in the audit trail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the escalation\n",
    "\n",
    "escalation_body = {\n",
    "    \"severity\": \"HIGH\",\n",
    "    \"reason\": (\n",
    "        \"Detected anomalous reflectance pattern in grid cell 6 that could indicate \"\n",
    "        \"either (a) genuine flash flood extent beyond historical norms or (b) SAR \"\n",
    "        \"sensor artifact from wind-roughened water surface. Confidence in flood \"\n",
    "        \"classification: 0.62 -- below the 0.75 threshold for autonomous reporting. \"\n",
    "        \"Requesting human review of SAR backscatter imagery before finalizing results.\"\n",
    "    ),\n",
    "    \"context\": {\n",
    "        \"grid_cell\": 6,\n",
    "        \"confidence\": 0.62,\n",
    "        \"threshold\": 0.75,\n",
    "        \"possible_causes\": [\n",
    "            \"genuine_flood_extent\",\n",
    "            \"wind_roughened_surface_artifact\",\n",
    "        ],\n",
    "        \"recommended_action\": \"Review SAR backscatter in QuickLook viewer\",\n",
    "    },\n",
    "}\n",
    "\n",
    "ESCALATION_ID = None\n",
    "\n",
    "if JOB_ID:\n",
    "    sc, body = api_post(f\"/control/v1/jobs/{JOB_ID}/escalations\", escalation_body)\n",
    "    print(f\"HTTP {sc}\")\n",
    "    pp(body)\n",
    "    \n",
    "    if isinstance(body, dict):\n",
    "        ESCALATION_ID = body.get(\"escalation_id\")\n",
    "        print(f\"\\nEscalation ID: {ESCALATION_ID}\")\n",
    "        print(f\"Severity:      {body.get('severity')}\")\n",
    "else:\n",
    "    print(\"No job ID available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve the escalation -- human operator confirms genuine flood extent\n",
    "\n",
    "resolve_body = {\n",
    "    \"resolution\": (\n",
    "        \"Reviewed SAR backscatter in QuickLook viewer. Pattern in grid cell 6 is \"\n",
    "        \"consistent with genuine flood extent -- the irregular boundary matches the \"\n",
    "        \"Greens Bayou overflow pattern from historical events. Wind artifact ruled out \"\n",
    "        \"based on VV/VH polarization ratio analysis. Classification confidence upgraded \"\n",
    "        \"to 0.91. Approved for reporting.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "if JOB_ID and ESCALATION_ID:\n",
    "    sc, body = api_patch(\n",
    "        f\"/control/v1/jobs/{JOB_ID}/escalations/{ESCALATION_ID}\",\n",
    "        resolve_body,\n",
    "    )\n",
    "    print(f\"HTTP {sc}\")\n",
    "    pp(body)\n",
    "    \n",
    "    if isinstance(body, dict) and body.get(\"resolved_at\"):\n",
    "        print(f\"\\nEscalation resolved at: {body['resolved_at']}\")\n",
    "else:\n",
    "    print(\"No escalation to resolve.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete the Pipeline\n",
    "\n",
    "With the escalation resolved and confidence upgraded, we walk through the final pipeline\n",
    "stages to **COMPLETE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final transitions: QUALITY_CHECK --> ANALYZED --> REPORTING --> ASSEMBLING --> REPORTED --> COMPLETE\n",
    "\n",
    "FINAL_TRANSITIONS = [\n",
    "    {\n",
    "        \"expected_phase\": \"ANALYZING\", \"expected_status\": \"QUALITY_CHECK\",\n",
    "        \"target_phase\": \"ANALYZING\", \"target_status\": \"ANALYZED\",\n",
    "        \"reason\": \"Analysis complete -- flood extent map generated\",\n",
    "    },\n",
    "    {\n",
    "        \"expected_phase\": \"ANALYZING\", \"expected_status\": \"ANALYZED\",\n",
    "        \"target_phase\": \"REPORTING\", \"target_status\": \"REPORTING\",\n",
    "        \"reason\": \"Generating analysis products and report\",\n",
    "    },\n",
    "    {\n",
    "        \"expected_phase\": \"REPORTING\", \"expected_status\": \"REPORTING\",\n",
    "        \"target_phase\": \"REPORTING\", \"target_status\": \"ASSEMBLING\",\n",
    "        \"reason\": \"Assembling final deliverables (GeoTIFF, GeoJSON, PDF)\",\n",
    "    },\n",
    "    {\n",
    "        \"expected_phase\": \"REPORTING\", \"expected_status\": \"ASSEMBLING\",\n",
    "        \"target_phase\": \"REPORTING\", \"target_status\": \"REPORTED\",\n",
    "        \"reason\": \"Report assembled and validated\",\n",
    "    },\n",
    "    {\n",
    "        \"expected_phase\": \"REPORTING\", \"expected_status\": \"REPORTED\",\n",
    "        \"target_phase\": \"COMPLETE\", \"target_status\": \"COMPLETE\",\n",
    "        \"reason\": \"Job complete -- results published to STAC catalog\",\n",
    "    },\n",
    "]\n",
    "\n",
    "if JOB_ID:\n",
    "    # Inject final reasoning before completing\n",
    "    reasoning_final = {\n",
    "        \"reasoning\": (\n",
    "            \"Flood detection analysis complete for Houston Ship Channel area. \"\n",
    "            \"Key findings: (1) Buffalo Bayou overflow detected along 4.2 km stretch \"\n",
    "            \"between Waugh Dr and US-59. (2) Estimated flood extent: 3.8 km2. \"\n",
    "            \"(3) 94 structures identified within flood boundary using building footprint \"\n",
    "            \"overlay. (4) SAR coherence method corroborated by NDWI optical analysis with \"\n",
    "            \"87% spatial agreement. Confidence in overall assessment: 0.91.\"\n",
    "        ),\n",
    "        \"confidence\": 0.91,\n",
    "        \"payload\": {\n",
    "            \"flood_extent_km2\": 3.8,\n",
    "            \"affected_structures\": 94,\n",
    "            \"sar_optical_agreement\": 0.87,\n",
    "            \"primary_method\": \"SAR coherence change detection\",\n",
    "            \"corroboration\": \"NDWI optical thresholding\",\n",
    "        },\n",
    "    }\n",
    "    sc, body = api_post(f\"/control/v1/jobs/{JOB_ID}/reasoning\", reasoning_final)\n",
    "    print(f\"Final reasoning recorded: HTTP {sc}\")\n",
    "    if isinstance(body, dict):\n",
    "        print(f\"  Event sequence: {body.get('event_seq')}\")\n",
    "    print()\n",
    "    \n",
    "    # Walk through final transitions\n",
    "    for t in FINAL_TRANSITIONS:\n",
    "        sc, body = api_post(f\"/control/v1/jobs/{JOB_ID}/transition\", t)\n",
    "        phase = body.get(\"phase\", \"?\") if isinstance(body, dict) else \"?\"\n",
    "        status_val = body.get(\"status\", \"?\") if isinstance(body, dict) else \"?\"\n",
    "        marker = \"OK\" if 200 <= sc < 300 else f\"ERR {sc}\"\n",
    "        print(f\"  [{marker}] {phase}/{status_val} -- {t['reason']}\")\n",
    "    \n",
    "    print(f\"\\nPipeline COMPLETE.\")\n",
    "else:\n",
    "    print(\"No job ID available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Context Lakehouse\n",
    "\n",
    "The Context Lakehouse is PostGIS-backed storage that accumulates **all contextual data**\n",
    "across analysis jobs -- satellite datasets, building footprints, critical infrastructure,\n",
    "and weather observations.\n",
    "\n",
    "This is what turns individual analyses into **compound intelligence**. When a second flood\n",
    "event hits the same area, the satellite scenes and building data from the first analysis\n",
    "are already cached -- reducing ingestion time and enabling temporal comparisons.\n",
    "\n",
    "All endpoints accept **spatial filters** (bbox) and return paginated results.\n",
    "\n",
    "> **Why This Matters:** MAIA does not just get analysis results -- it gets the raw spatial\n",
    "> context to perform its own reasoning. \"37 buildings in the flood extent\", \"2 hospitals\n",
    "> within 2 km\", \"12,400 population affected\" -- these decision products come from\n",
    "> spatial joins against the lakehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8a. Query Datasets\n",
    "\n",
    "**`GET /control/v1/context/datasets`**\n",
    "\n",
    "Returns accumulated satellite scene metadata: source catalog, footprint geometry, bands,\n",
    "resolution, cloud cover, acquisition date. Supports spatial (bbox) and temporal (date range)\n",
    "filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query datasets in the Houston AOI bbox\n",
    "HOUSTON_BBOX = \"-95.3698,29.7100,-95.2900,29.7604\"\n",
    "\n",
    "sc, body = api_get(\"/control/v1/context/datasets\", params={\"bbox\": HOUSTON_BBOX, \"page_size\": 5})\n",
    "print(f\"HTTP {sc}\")\n",
    "pp(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Query Buildings\n",
    "\n",
    "**`GET /control/v1/context/buildings`**\n",
    "\n",
    "Returns building footprints from OpenStreetMap and other sources within the specified bbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc, body = api_get(\"/control/v1/context/buildings\", params={\"bbox\": HOUSTON_BBOX, \"page_size\": 5})\n",
    "print(f\"HTTP {sc}\")\n",
    "pp(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8c. Query Critical Infrastructure\n",
    "\n",
    "**`GET /control/v1/context/infrastructure`**\n",
    "\n",
    "Returns hospitals, fire stations, schools, and other critical facilities. Supports\n",
    "filtering by type (e.g., `?type=hospital`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc, body = api_get(\"/control/v1/context/infrastructure\", params={\"bbox\": HOUSTON_BBOX, \"page_size\": 5})\n",
    "print(f\"HTTP {sc}\")\n",
    "pp(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8d. Query Weather Observations\n",
    "\n",
    "**`GET /control/v1/context/weather`**\n",
    "\n",
    "Returns weather observations (rain gauges, NOAA alerts, station data) with spatial and\n",
    "temporal filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc, body = api_get(\"/control/v1/context/weather\", params={\"bbox\": HOUSTON_BBOX, \"page_size\": 5})\n",
    "print(f\"HTTP {sc}\")\n",
    "pp(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8e. Lakehouse Summary\n",
    "\n",
    "**`GET /control/v1/context/summary`**\n",
    "\n",
    "Returns aggregate statistics across the entire lakehouse: row counts per table,\n",
    "spatial extent, distinct data sources, and usage statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc, body = api_get(\"/control/v1/context/summary\")\n",
    "print(f\"HTTP {sc}\")\n",
    "pp(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8f. Per-Job Context Usage\n",
    "\n",
    "**`GET /control/v1/jobs/{job_id}/context`**\n",
    "\n",
    "Shows how much context data was **ingested** (fetched fresh) versus **reused** (already in\n",
    "the lakehouse from a previous job) for a specific job. This is the key metric for the\n",
    "lakehouse effect -- over time, reuse increases and ingestion costs decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if JOB_ID:\n",
    "    sc, body = api_get(f\"/control/v1/jobs/{JOB_ID}/context\")\n",
    "    print(f\"HTTP {sc}\")\n",
    "    pp(body)\n",
    "else:\n",
    "    print(\"No job ID available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Partner Integration: Metrics & Queue\n",
    "\n",
    "These endpoints are designed for MAIA's **operations dashboard** -- real-time visibility\n",
    "into pipeline health.\n",
    "\n",
    "Metrics are served from **materialized views** that refresh every 30 seconds. This means\n",
    "responses come back in under 100ms regardless of how many jobs are in the system -- the\n",
    "dashboard can poll every few seconds without impacting API performance.\n",
    "\n",
    "> **Why This Matters:** MAIA's operations team needs to know at a glance: How many jobs\n",
    "> completed? How many failed? What is the P95 latency? Are any jobs stuck? These\n",
    "> endpoints provide that view with zero computation at query time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9a. Pipeline Health Metrics\n",
    "\n",
    "**`GET /internal/v1/metrics`**\n",
    "\n",
    "Aggregated metrics: jobs completed/failed (1h and 24h windows), P50/P95 job duration,\n",
    "active SSE connections, and webhook delivery success rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc, body = api_get(\"/internal/v1/metrics\")\n",
    "print(f\"HTTP {sc}\")\n",
    "pp(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9b. Queue Summary\n",
    "\n",
    "**`GET /internal/v1/queue/summary`**\n",
    "\n",
    "Per-phase job counts, stuck job count, and jobs awaiting human review (open escalations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc, body = api_get(\"/internal/v1/queue/summary\")\n",
    "print(f\"HTTP {sc}\")\n",
    "pp(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. SSE Event Stream\n",
    "\n",
    "**`GET /internal/v1/events/stream`**\n",
    "\n",
    "FirstLight provides a **Server-Sent Events (SSE)** endpoint for real-time event delivery.\n",
    "Every state transition, reasoning entry, escalation, and parameter change is broadcast\n",
    "as a **CloudEvents v1.0** envelope.\n",
    "\n",
    "### Capabilities\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Last-Event-ID** | Reconnect and replay missed events from a specific sequence number |\n",
    "| **customer_id scoping** | Multi-tenant isolation -- each customer only sees their events |\n",
    "| **Event type filtering** | `?type=job.transition` to subscribe to specific event types |\n",
    "| **Backpressure** | Max 500 buffered events per connection -- slow consumers are disconnected gracefully |\n",
    "| **Heartbeat** | 30-second keepalive comments to detect dead connections |\n",
    "\n",
    "The SSE endpoint is best consumed from a **backend service or CLI** rather than a notebook.\n",
    "Here is how to connect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SSE is a long-lived streaming connection -- not practical to run in a notebook cell.\n# Here is how to connect from a backend or CLI:\n\ncurl_command = f\"\"\"# Connect to SSE event stream (run in terminal)\ncurl -N -H \"X-API-Key: {API_KEY}\" \\\\\n     -H \"Accept: text/event-stream\" \\\\\n     \"{BASE_URL}/internal/v1/events/stream\"\n\n# With Last-Event-ID for replay after reconnection:\ncurl -N -H \"X-API-Key: {API_KEY}\" \\\\\n     -H \"Accept: text/event-stream\" \\\\\n     -H \"Last-Event-ID: 42\" \\\\\n     \"{BASE_URL}/internal/v1/events/stream\"\n\n# Filtered by event type:\ncurl -N -H \"X-API-Key: {API_KEY}\" \\\\\n     -H \"Accept: text/event-stream\" \\\\\n     \"{BASE_URL}/internal/v1/events/stream?type=job.transition\"\"\"\n\nprint(curl_command)\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"CloudEvents v1.0 envelope example:\")\nprint(\"=\" * 70)\n\nexample_event = {\n    \"specversion\": \"1.0\",\n    \"id\": \"evt-42\",\n    \"source\": \"firstlight/control-plane\",\n    \"type\": \"job.transition\",\n    \"subject\": JOB_ID or \"<job-uuid>\",\n    \"time\": datetime.now(timezone.utc).isoformat(),\n    \"datacontenttype\": \"application/json\",\n    \"data\": {\n        \"job_id\": JOB_ID or \"<job-uuid>\",\n        \"phase\": \"ANALYZING\",\n        \"status\": \"QUALITY_CHECK\",\n        \"actor\": \"maia-agent\",\n        \"reasoning\": \"Running QA checks on detection output\",\n    },\n}\npp(example_event)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webhook Registration\n",
    "\n",
    "For push-based delivery (instead of SSE pull), MAIA can register webhooks:\n",
    "\n",
    "| Method | Endpoint | Description |\n",
    "|--------|---------|-------------|\n",
    "| `POST` | `/internal/v1/webhooks` | Register a new webhook (HTTPS required, SSRF-protected) |\n",
    "| `GET` | `/internal/v1/webhooks` | List registered webhooks (cursor-based pagination) |\n",
    "| `DELETE` | `/internal/v1/webhooks/{id}` | Deregister a webhook |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List existing webhook subscriptions\n",
    "sc, body = api_get(\"/internal/v1/webhooks\")\n",
    "print(f\"HTTP {sc}\")\n",
    "pp(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Standards Integration: OGC & STAC\n",
    "\n",
    "FirstLight does not lock data into a proprietary format. The same algorithms and results\n",
    "are accessible through **open geospatial standards**.\n",
    "\n",
    "### OGC API Processes (`/oapi/*`)\n",
    "\n",
    "The same flood detection algorithms available through the LLM Router are also exposed\n",
    "as OGC API Processes. This means QGIS, ArcGIS Pro, and any OGC-compliant client can\n",
    "submit and monitor jobs without using the LLM Control Plane.\n",
    "\n",
    "### STAC Catalog (`/stac/*`)\n",
    "\n",
    "Every completed analysis is published as a STAC Item. The STAC catalog makes results\n",
    "discoverable by spatial and temporal search -- standard practice in the Earth observation\n",
    "community.\n",
    "\n",
    "> **Why This Matters:** MAIA is not the only consumer. Emergency managers use QGIS.\n",
    "> Researchers use Python + pystac. GIS teams use ArcGIS. Open standards mean FirstLight\n",
    "> fits into existing workflows without lock-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check OGC API Processes\n",
    "print(\"OGC API Processes (/oapi/processes):\")\n",
    "print(\"=\" * 50)\n",
    "sc, body = api_get(\"/oapi/processes\")\n",
    "print(f\"HTTP {sc}\")\n",
    "if sc == 200:\n",
    "    if isinstance(body, dict):\n",
    "        processes = body.get(\"processes\", [])\n",
    "        print(f\"Available processes: {len(processes)}\")\n",
    "        for p in processes[:5]:\n",
    "            print(f\"  - {p.get('id', '?')}: {p.get('title', p.get('description', ''))[:60]}\")\n",
    "    else:\n",
    "        pp(body)\n",
    "elif sc == 404:\n",
    "    print(\"OGC endpoint not deployed on this instance (pygeoapi optional).\")\n",
    "else:\n",
    "    pp(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check STAC Catalog\n",
    "print(\"STAC Catalog (/stac):\")\n",
    "print(\"=\" * 50)\n",
    "sc, body = api_get(\"/stac\")\n",
    "print(f\"HTTP {sc}\")\n",
    "if sc == 200:\n",
    "    pp(body)\n",
    "elif sc == 404:\n",
    "    print(\"STAC endpoint not deployed on this instance (stac-fastapi optional).\")\n",
    "else:\n",
    "    pp(body)\n",
    "\n",
    "print(\"\\nSTAC Collections (/stac/collections):\")\n",
    "print(\"=\" * 50)\n",
    "sc, body = api_get(\"/stac/collections\")\n",
    "print(f\"HTTP {sc}\")\n",
    "if sc == 200 and isinstance(body, dict):\n",
    "    collections = body.get(\"collections\", [])\n",
    "    print(f\"Collections: {len(collections)}\")\n",
    "    for c in collections[:5]:\n",
    "        print(f\"  - {c.get('id')}: {c.get('title', c.get('description', ''))[:60]}\")\n",
    "elif sc == 404:\n",
    "    print(\"STAC collections not yet available.\")\n",
    "else:\n",
    "    pp(body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## 12. Summary\n\n### What We Demonstrated\n\n| # | Capability | What Happened |\n|---|-----------|---------------|\n| 1 | **Tool Discovery** | AI agent discovered available algorithms at runtime via OpenAI-compatible schemas |\n| 2 | **Job Creation** | Submitted flood analysis with GeoJSON AOI + initial reasoning |\n| 3 | **Phase Transitions** | Walked through 7 phases with atomic TOCTOU-guarded state transitions |\n| 4 | **Reasoning Injection** | AI recorded chain of thought with confidence scores and structured payloads |\n| 5 | **Parameter Tuning** | Adjusted algorithm sensitivity mid-flight via JSON merge-patch |\n| 6 | **Escalation Workflow** | Flagged low-confidence finding for human review, then resolved it |\n| 7 | **Context Lakehouse** | Queried accumulated spatial data across datasets, buildings, infrastructure, weather |\n| 8 | **Metrics & Queue** | Retrieved pipeline health and queue status from materialized views |\n| 9 | **SSE Events** | Showed real-time event streaming with CloudEvents v1.0 envelopes |\n| 10 | **OGC & STAC** | Verified standards-compliant interfaces for GIS tool interoperability |\n\n### Complete Endpoint Map\n\n#### LLM Control Plane (`/control/v1`)\n\n| Method | Endpoint | Description |\n|--------|---------|-------------|\n| `GET` | `/control/v1/tools` | Discover algorithm tool schemas (OpenAI format) |\n| `POST` | `/control/v1/jobs` | Create a new analysis job |\n| `GET` | `/control/v1/jobs` | List jobs (filter by phase, status, event_type, bbox) |\n| `GET` | `/control/v1/jobs/{id}` | Get full job detail |\n| `POST` | `/control/v1/jobs/{id}/transition` | Atomic state transition (TOCTOU guard) |\n| `POST` | `/control/v1/jobs/{id}/reasoning` | Inject LLM reasoning entry |\n| `PATCH` | `/control/v1/jobs/{id}/parameters` | Tune parameters mid-flight (merge-patch) |\n| `POST` | `/control/v1/jobs/{id}/escalations` | Create escalation |\n| `PATCH` | `/control/v1/jobs/{id}/escalations/{eid}` | Resolve escalation |\n| `GET` | `/control/v1/jobs/{id}/escalations` | List escalations for a job |\n| `GET` | `/control/v1/jobs/{id}/context` | Per-job context usage (ingested vs reused) |\n\n#### Context Lakehouse (`/control/v1/context`)\n\n| Method | Endpoint | Description |\n|--------|---------|-------------|\n| `GET` | `/control/v1/context/datasets` | Query satellite datasets (bbox, date range, source) |\n| `GET` | `/control/v1/context/buildings` | Query building footprints (bbox) |\n| `GET` | `/control/v1/context/infrastructure` | Query critical infrastructure (bbox, type) |\n| `GET` | `/control/v1/context/weather` | Query weather observations (bbox, time range) |\n| `GET` | `/control/v1/context/summary` | Lakehouse-wide statistics |\n\n#### Partner Integration (`/internal/v1`)\n\n| Method | Endpoint | Description |\n|--------|---------|-------------|\n| `GET` | `/internal/v1/events/stream` | SSE event stream (CloudEvents v1.0) |\n| `GET` | `/internal/v1/metrics` | Pipeline health metrics (materialized view) |\n| `GET` | `/internal/v1/queue/summary` | Queue status (per-phase counts, stuck jobs) |\n| `POST` | `/internal/v1/webhooks` | Register webhook subscription |\n| `GET` | `/internal/v1/webhooks` | List webhook subscriptions |\n| `DELETE` | `/internal/v1/webhooks/{id}` | Deregister webhook |\n\n#### Standards\n\n| Method | Endpoint | Description |\n|--------|---------|-------------|\n| `GET` | `/oapi/processes` | OGC API Processes (standards-compliant algorithms) |\n| `GET` | `/stac/collections` | STAC Catalog (published analysis results) |\n| `GET` | `/api/v1/health` | Platform health check |\n\n### Architecture Highlights\n\n- **PostGIS** stores all spatial state -- jobs, context data, events. Every geometry is indexed\n  for fast spatial queries.\n- **Materialized views** power the metrics and queue endpoints -- pre-computed every 30 seconds,\n  so dashboard polling is effectively free.\n- **CloudEvents v1.0** envelopes on the SSE stream ensure interoperability with any event-driven\n  architecture.\n- **TOCTOU guards** on state transitions prevent race conditions in multi-agent environments.\n- **X-API-Key auth** with tenant-scoped data isolation -- MAIA sees only MAIA's jobs.\n- **JSON merge-patch** for parameter updates follows RFC 7386 -- no custom protocol.\n\n### Key Differentiators\n\n1. **LLM-Native** -- Built for AI agents from day one. Tool discovery, reasoning injection,\n   and confidence-gated escalation are first-class features, not afterthoughts.\n\n2. **Context Lakehouse** -- Accumulated spatial data compounds across jobs. The second analysis\n   in an area is faster and richer than the first.\n\n3. **Open Standards** -- OGC API Processes + STAC + CloudEvents. No vendor lock-in.\n   GIS tools, AI agents, and custom dashboards all consume the same data.\n\n4. **Full Audit Trail** -- Every decision, transition, reasoning entry, and escalation is\n   recorded as an immutable event. Complete explainability from trigger to deliverable."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final status check on our demo job\n",
    "if JOB_ID:\n",
    "    sc, detail = api_get(f\"/control/v1/jobs/{JOB_ID}\")\n",
    "    if sc == 200 and isinstance(detail, dict):\n",
    "        print(f\"Demo Job Summary\")\n",
    "        print(f\"================\")\n",
    "        print(f\"  Job ID:     {detail.get('job_id')}\")\n",
    "        print(f\"  Phase:      {detail.get('phase')}\")\n",
    "        print(f\"  Status:     {detail.get('status')}\")\n",
    "        print(f\"  Event Type: {detail.get('event_type')}\")\n",
    "        print(f\"  AOI Area:   {detail.get('aoi_area_km2')} km2\")\n",
    "        print(f\"  Created:    {detail.get('created_at')}\")\n",
    "        print(f\"  Updated:    {detail.get('updated_at')}\")\n",
    "    else:\n",
    "        print(f\"HTTP {sc}\")\n",
    "        pp(detail)\n",
    "else:\n",
    "    print(\"No job was created during this session.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"  Demo complete. Questions?\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}